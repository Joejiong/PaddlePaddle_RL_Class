{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "大作业：四轴飞行器悬浮\n",
    "【作业五 大作业评分标准】\n",
    "（需要保留notebook上每个cell运行之后的log信息）：\n",
    "1、完全没有log信息，59分\n",
    "2、完成部分函数的编写，有部分log信息，但代码运行失败，70分\n",
    "3、代码运行成功，但Test reward收敛效果不好（所有分数均低于6000），80分\n",
    "4、代码运行成功，Test reward有上涨趋势，且最后的Evaluate reward在[6000, 8000]之间，90分\n",
    "5、代码运行成功，Test reward有上涨趋势，且最后的Evaluate reward大于8000，100分\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping \n",
    "from parl.utils import ReplayMemory \n",
    "from rlschool import make_env \n",
    "from parl.algorithms import DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR  = 0.0005        # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 0.005         # Critic网络更新的 learning rate\n",
    "GAMMA = 0.9               # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU   = 0.001             # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE        = 1e4  # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e4  # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "REWARD_SCALE = 0.01       # reward 的缩放因子\n",
    "BATCH_SIZE   = 256        # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6   # 总训练步数\n",
    "TEST_EVERY_STEPS  = 1e4   # 每个N步评估一下算法效果，每次评估5个episode求平均reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        self.fc1 = layers.fc(size=64, act='relu')\n",
    "        self.fc2 = layers.fc(size=64, act='relu')\n",
    "        self.fc3 = layers.fc(size=act_dim, act='tanh')\n",
    "\n",
    "    def policy(self, obs):\n",
    "        hidden1 = self.fc1(obs)\n",
    "        hidden2 = self.fc2(hidden1)\n",
    "        logits  = self.fc3(hidden2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        hidden_size = 64\n",
    "        self.fc1 = layers.fc(size=hidden_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hidden_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=1, act=None)\n",
    "    def value(self, obs, act):\n",
    "        concat = layers.concat([obs, act], axis=1)\n",
    "        hidden1 = self.fc1(concat)\n",
    "        hidden2 = self.fc2(hidden1)\n",
    "        Q = self.fc3(hidden2)\n",
    "        Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadrotorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        self.actor_model = ActorModel(act_dim)\n",
    "        self.critic_model = CriticModel()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadrotorAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim=4):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(QuadrotorAgent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意：最开始先同步self.model和self.target_model的参数.\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        act = np.squeeze(act)\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "        return critic_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "        action = np.squeeze(action)\n",
    "        action = np.clip(np.random.normal(action, 1.0), -1.0, 1.0)\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, \\\n",
    "                    batch_terminal = rpm.sample_batch(BATCH_SIZE)\n",
    "            critic_cost = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                      batch_next_obs, batch_terminal)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward, steps = 0, 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            action = np.squeeze(action)\n",
    "            action = np.clip(action, -1.0, 1.0)\n",
    "            action = action_mapping(action, env.action_space.low[0], \n",
    "                                    env.action_space.high[0])\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-26 18:40:31 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[06-26 18:40:31 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[06-26 18:40:31 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n"
     ]
    }
   ],
   "source": [
    "# 创建飞行器环境\n",
    "env = make_env(\"Quadrotor\", task=\"hovering_control\")\n",
    "env.reset()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "model = QuadrotorModel(act_dim)\n",
    "algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "agent.restore('./model_dir/Pre_Training.ckpt')\n",
    "\n",
    "rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-26 18:41:38 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 1000, Test reward: 8457.575368518274\n",
      "\u001b[32m[06-26 18:43:26 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 10000, Test reward: 8413.648278417646\n",
      "\u001b[32m[06-26 18:45:21 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 20000, Test reward: 8274.117524445575\n",
      "\u001b[32m[06-26 18:47:15 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 30000, Test reward: 8433.745273897543\n",
      "\u001b[32m[06-26 18:49:09 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 40000, Test reward: 8491.882887179145\n",
      "\u001b[32m[06-26 18:51:04 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 50000, Test reward: 8423.17295522265\n",
      "\u001b[32m[06-26 18:52:58 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 60000, Test reward: 8484.636218737905\n",
      "\u001b[32m[06-26 18:54:53 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 70000, Test reward: 8555.556190381061\n",
      "\u001b[32m[06-26 18:56:48 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 80000, Test reward: 8308.305733207244\n",
      "\u001b[32m[06-26 18:58:43 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 90000, Test reward: 8352.215185358353\n",
      "\u001b[32m[06-26 19:00:38 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 100000, Test reward: 8165.247595050711\n",
      "\u001b[32m[06-26 19:02:33 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 110000, Test reward: 8312.02901511712\n",
      "\u001b[32m[06-26 19:04:28 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 120000, Test reward: 8351.293351825097\n",
      "\u001b[32m[06-26 19:06:22 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 130000, Test reward: 8520.293984489672\n",
      "\u001b[32m[06-26 19:08:18 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 140000, Test reward: 8116.257249201144\n",
      "\u001b[32m[06-26 19:10:13 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 150000, Test reward: 7940.965732319763\n",
      "\u001b[32m[06-26 19:12:08 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 160000, Test reward: 8309.562177846035\n",
      "\u001b[32m[06-26 19:14:03 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 170000, Test reward: 8434.84518881406\n",
      "\u001b[32m[06-26 19:15:58 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 180000, Test reward: 8401.213064948955\n",
      "\u001b[32m[06-26 19:17:53 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 190000, Test reward: 8347.160157870216\n",
      "\u001b[32m[06-26 19:19:48 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 200000, Test reward: 8532.785353363925\n",
      "\u001b[32m[06-26 19:21:44 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 210000, Test reward: 7973.352919362874\n",
      "\u001b[32m[06-26 19:23:39 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 220000, Test reward: 8370.76858126609\n",
      "\u001b[32m[06-26 19:25:34 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 230000, Test reward: 8139.875385743115\n",
      "\u001b[32m[06-26 19:27:30 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 240000, Test reward: 8048.160733358831\n",
      "\u001b[32m[06-26 19:29:25 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 250000, Test reward: 8140.138982421937\n",
      "\u001b[32m[06-26 19:31:21 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 260000, Test reward: 7962.663311564992\n",
      "\u001b[32m[06-26 19:33:16 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 270000, Test reward: 8402.4281350067\n",
      "\u001b[32m[06-26 19:35:12 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 280000, Test reward: 7997.908669691725\n",
      "\u001b[32m[06-26 19:37:07 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 290000, Test reward: 7651.18801329874\n",
      "\u001b[32m[06-26 19:39:03 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 300000, Test reward: 7937.579875481504\n",
      "\u001b[32m[06-26 19:40:58 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 310000, Test reward: 8450.466175051271\n",
      "\u001b[32m[06-26 19:42:53 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 320000, Test reward: 8392.753658108346\n",
      "\u001b[32m[06-26 19:44:49 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 330000, Test reward: 8071.92859784765\n",
      "\u001b[32m[06-26 19:46:45 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 340000, Test reward: 8536.884238039787\n",
      "\u001b[32m[06-26 19:48:40 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 350000, Test reward: 8387.085402231998\n",
      "\u001b[32m[06-26 19:50:36 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 360000, Test reward: 7806.476278956715\n",
      "\u001b[32m[06-26 19:52:32 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 370000, Test reward: 8007.372596444606\n",
      "\u001b[32m[06-26 19:54:27 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 380000, Test reward: 8200.010000775665\n",
      "\u001b[32m[06-26 19:56:23 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 390000, Test reward: 8314.787285775095\n",
      "\u001b[32m[06-26 19:58:18 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 400000, Test reward: 8199.27456843392\n",
      "\u001b[32m[06-26 20:00:13 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 410000, Test reward: 8515.277227544193\n",
      "\u001b[32m[06-26 20:02:09 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 420000, Test reward: 8119.561939216745\n",
      "\u001b[32m[06-26 20:04:04 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 430000, Test reward: 8268.546412635123\n",
      "\u001b[32m[06-26 20:06:00 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 440000, Test reward: 8526.188400317162\n",
      "\u001b[32m[06-26 20:07:56 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 450000, Test reward: 8206.293657444296\n",
      "\u001b[32m[06-26 20:09:51 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 460000, Test reward: 8412.594629959145\n",
      "\u001b[32m[06-26 20:11:47 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 470000, Test reward: 8052.723150378704\n",
      "\u001b[32m[06-26 20:13:42 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 480000, Test reward: 8311.126868266987\n",
      "\u001b[32m[06-26 20:15:38 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 490000, Test reward: 8252.092921937892\n",
      "\u001b[32m[06-26 20:17:33 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 500000, Test reward: 8471.434349793433\n",
      "\u001b[32m[06-26 20:19:29 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 510000, Test reward: 8344.429456679476\n",
      "\u001b[32m[06-26 20:21:24 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 520000, Test reward: 8520.442250802336\n",
      "\u001b[32m[06-26 20:23:20 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 530000, Test reward: 8300.023617747569\n",
      "\u001b[32m[06-26 20:25:15 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 540000, Test reward: 8503.446385955438\n",
      "\u001b[32m[06-26 20:27:11 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 550000, Test reward: 7964.849812898812\n",
      "\u001b[32m[06-26 20:29:07 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 560000, Test reward: 7903.1014825646735\n",
      "\u001b[32m[06-26 20:31:02 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 570000, Test reward: 8543.899756204206\n",
      "\u001b[32m[06-26 20:32:58 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 580000, Test reward: 8166.158828452933\n",
      "\u001b[32m[06-26 20:34:54 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 590000, Test reward: 8313.87593231354\n",
      "\u001b[32m[06-26 20:36:49 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 600000, Test reward: 8173.003078583691\n",
      "\u001b[32m[06-26 20:38:45 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 610000, Test reward: 8343.63632471075\n",
      "\u001b[32m[06-26 20:40:41 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 620000, Test reward: 8385.698182566994\n",
      "\u001b[32m[06-26 20:42:36 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 630000, Test reward: 8377.337274434314\n",
      "\u001b[32m[06-26 20:44:32 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 640000, Test reward: 8486.651532365451\n",
      "\u001b[32m[06-26 20:46:28 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 650000, Test reward: 8450.624014728248\n",
      "\u001b[32m[06-26 20:48:24 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 660000, Test reward: 8255.217429583146\n",
      "\u001b[32m[06-26 20:50:19 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 670000, Test reward: 8506.40852659599\n",
      "\u001b[32m[06-26 20:52:15 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 680000, Test reward: 8460.995361000318\n",
      "\u001b[32m[06-26 20:54:11 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 690000, Test reward: 7994.6098211103235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-26 20:56:07 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 700000, Test reward: 8302.525390305636\n",
      "\u001b[32m[06-26 20:58:02 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 710000, Test reward: 8535.029504433778\n",
      "\u001b[32m[06-26 20:59:58 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 720000, Test reward: 8343.93996737016\n",
      "\u001b[32m[06-26 21:01:49 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 730000, Test reward: 8176.534532100722\n",
      "\u001b[32m[06-26 21:03:37 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 740000, Test reward: 8242.895155016737\n",
      "\u001b[32m[06-26 21:05:23 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 750000, Test reward: 8350.851381578424\n",
      "\u001b[32m[06-26 21:07:10 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 760000, Test reward: 8329.807101654656\n",
      "\u001b[32m[06-26 21:08:56 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 770000, Test reward: 8358.315287197142\n",
      "\u001b[32m[06-26 21:10:43 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 780000, Test reward: 8085.545040758535\n",
      "\u001b[32m[06-26 21:12:30 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 790000, Test reward: 8250.141344364087\n",
      "\u001b[32m[06-26 21:14:17 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 800000, Test reward: 8502.900901779301\n",
      "\u001b[32m[06-26 21:16:03 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 810000, Test reward: 8354.52083752412\n",
      "\u001b[32m[06-26 21:17:50 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 820000, Test reward: 8559.786772789987\n",
      "\u001b[32m[06-26 21:19:36 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 830000, Test reward: 7895.968586433754\n",
      "\u001b[32m[06-26 21:21:23 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 840000, Test reward: 8250.506723353514\n",
      "\u001b[32m[06-26 21:23:10 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 850000, Test reward: 8309.080158104305\n",
      "\u001b[32m[06-26 21:24:56 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 860000, Test reward: 8389.669413140533\n",
      "\u001b[32m[06-26 21:26:43 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 870000, Test reward: 8370.42265872794\n",
      "\u001b[32m[06-26 21:28:30 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 880000, Test reward: 8236.332013445292\n",
      "\u001b[32m[06-26 21:30:17 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 890000, Test reward: 8171.833199888753\n",
      "\u001b[32m[06-26 21:32:04 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 900000, Test reward: 8495.717902653205\n",
      "\u001b[32m[06-26 21:33:50 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 910000, Test reward: 8144.593028309684\n",
      "\u001b[32m[06-26 21:35:37 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 920000, Test reward: 8278.709518482177\n",
      "\u001b[32m[06-26 21:37:24 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 930000, Test reward: 7809.789359005472\n",
      "\u001b[32m[06-26 21:39:10 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 940000, Test reward: 8332.772546700107\n",
      "\u001b[32m[06-26 21:40:57 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 950000, Test reward: 8552.691679730056\n",
      "\u001b[32m[06-26 21:42:44 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 960000, Test reward: 8119.865416324481\n",
      "\u001b[32m[06-26 21:44:30 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 970000, Test reward: 8478.941058081076\n",
      "\u001b[32m[06-26 21:46:18 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 980000, Test reward: 8404.74369765039\n",
      "\u001b[32m[06-26 21:48:06 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 990000, Test reward: 8393.284613048016\n",
      "\u001b[32m[06-26 21:49:54 MainThread @<ipython-input-12-efce29af3bdd>:13]\u001b[0m Steps 1000000, Test reward: 8055.424777486589\n"
     ]
    }
   ],
   "source": [
    "#启动训练\n",
    "test_flag = 0\n",
    "total_steps = 0\n",
    "while total_steps < TRAIN_TOTAL_STEPS:\n",
    "    train_reward, steps = run_episode(env, agent, rpm)\n",
    "    total_steps += steps\n",
    "    #logger.info('Steps: {} Reward: {}'.format(total_steps, train_reward)) # 打印训练reward\n",
    "    if total_steps // TEST_EVERY_STEPS >= test_flag: # 每隔一定step数，评估一次模型\n",
    "        while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "                test_flag += 1\n",
    "        evaluate_reward = evaluate(env, agent)\n",
    "        logger.info('Steps {}, Test reward: {}'.format(\n",
    "                total_steps, evaluate_reward)) # 打印评估的reward\n",
    "        # 每评估一次，就保存一次模型，以训练的step数命名\n",
    "        ckpt = 'model_dir/steps_{}.ckpt'.format(total_steps)\n",
    "        agent.save(ckpt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-26 22:27:58 MainThread @<ipython-input-13-8ba810ab151b>:5]\u001b[0m Evaluate reward: 8446.997822155343\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'model_dir/steps_1000000.ckpt'  # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\n",
    "\n",
    "agent.restore(ckpt)\n",
    "evaluate_reward = evaluate(env, agent)\n",
    "logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
