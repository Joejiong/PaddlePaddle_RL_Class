{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PG解决Pong\n",
    "作业4 PolicyGradient作业、评分标准（需要保留notebook上每个cell运行之后的log信息）：\n",
    "1、完全没有log信息，59分\n",
    "2、完成部分函数的编写，有部分log信息，但代码运行失败，70分\n",
    "3、代码运行成功，但Test reward收敛效果不好（所有分数均低于0），80分\n",
    "4、代码运行成功，分数有上涨趋势，且最后（Episode 2800、2900、3000）输出的3个Test reward的最高分在[0, 10]之间，90分\n",
    "5、代码运行成功，分数有上涨趋势，且最后（Episode 2800、2900、3000）输出的3个Test reward的最高分大于10，100分\n",
    "\n",
    "【作业4满分标准补充说明】\n",
    "1、最后10个分数一半以上大于10，100分\n",
    "2、如果最后10个分数表现不好，但是最近50个分数大部分都大于10，100分\n",
    "3、其他情况酌情给分\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from parl.algorithms import PolicyGradient \n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "from parl.utils import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        # 2. 请参考课程Demo，配置model结构\n",
    "        act_dim = act_dim\n",
    "        hidden_size = act_dim * 24\n",
    "        self.net1 = layers.fc(size=hidden_size, act='tanh')\n",
    "        self.net2 = layers.fc(size=act_dim, act='softmax')\n",
    "    def forward(self, obs):  # 可直接用 model = Model(5); model(obs)调用\n",
    "        # 3. 请参考课程Demo，组装policy网络\n",
    "        hidden1  = self.net1(obs)\n",
    "        out      = self.net2(hidden1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):  # 搭建计算图用于 预测动作，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.act_prob = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(\n",
    "                self.learn_program):  # 搭建计算图用于 更新policy网络，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(name='act', shape=[1], dtype='int64')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            self.cost = self.alg.learn(obs, act, reward)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)  # 增加一维维度\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)  # 减少一维维度\n",
    "        act = np.random.choice(range(self.act_dim), p=act_prob)  # 根据动作概率选取动作\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)\n",
    "        act = np.argmax(act_prob)  # 根据动作概率选择概率最高的动作\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward):\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        feed = {\n",
    "            'obs': obs.astype('float32'),\n",
    "            'act': act.astype('int64'),\n",
    "            'reward': reward.astype('float32')\n",
    "        }\n",
    "        cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.cost])[0]\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):\n",
    "    obs_list, action_list, reward_list = [], [], []\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs = preprocess(obs) # from shape (210, 160, 3) to (100800,)\n",
    "        obs_list.append(obs)\n",
    "        action = agent.sample(obs) # 采样动作\n",
    "        action_list.append(action)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return obs_list, action_list, reward_list\n",
    "\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            obs = preprocess(obs) # from shape (210, 160, 3) to (100800,)\n",
    "            action = agent.predict(obs) # 选取最优动作\n",
    "            obs, reward, isOver, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if isOver:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-26 18:42:30 MainThread @<ipython-input-10-c5112878137f>:29]\u001b[0m obs_dim 6400, act_dim 6\n",
      "\u001b[32m[06-26 18:42:30 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[06-26 18:42:30 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[06-26 18:48:28 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 100, Test reward: -17.0\n",
      "\u001b[32m[06-26 18:56:36 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 200, Test reward: -16.4\n",
      "\u001b[32m[06-26 19:05:47 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 300, Test reward: -15.4\n",
      "\u001b[32m[06-26 19:16:08 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 400, Test reward: -15.6\n",
      "\u001b[32m[06-26 19:27:54 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 500, Test reward: -15.6\n",
      "\u001b[32m[06-26 19:40:23 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 600, Test reward: -13.6\n",
      "\u001b[32m[06-26 19:53:33 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 700, Test reward: -14.8\n",
      "\u001b[32m[06-26 20:07:33 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 800, Test reward: -13.8\n",
      "\u001b[32m[06-26 20:22:28 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 900, Test reward: -12.0\n",
      "\u001b[32m[06-26 20:37:18 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1000, Test reward: -13.4\n",
      "\u001b[32m[06-26 20:52:19 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1100, Test reward: -15.0\n",
      "\u001b[32m[06-26 21:07:43 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1200, Test reward: -13.6\n",
      "\u001b[32m[06-26 21:23:23 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1300, Test reward: -11.8\n",
      "\u001b[32m[06-26 21:38:58 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1400, Test reward: -12.8\n",
      "\u001b[32m[06-26 21:54:24 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1500, Test reward: -9.0\n",
      "\u001b[32m[06-26 22:08:49 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1600, Test reward: -11.4\n",
      "\u001b[32m[06-26 22:23:24 MainThread @<ipython-input-10-c5112878137f>:57]\u001b[0m Episode 1700, Test reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "# Pong 图片预处理\n",
    "def preprocess(image):\n",
    "    \"\"\" 预处理 210x160x3 uint8 frame into 6400 (80x80) 1维 float vector \"\"\"\n",
    "    image = image[35:195] # 裁剪\n",
    "    image = image[::2,::2,0] # 下采样，缩放2倍\n",
    "    image[image == 144] = 0 # 擦除背景 (background type 1)\n",
    "    image[image == 109] = 0 # 擦除背景 (background type 2)\n",
    "    image[image != 0] = 1 # 转为灰度图，除了黑色外其他都是白色\n",
    "    return image.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "# 根据一个episode的每个step的reward列表，计算每一个Step的Gt\n",
    "def calc_reward_to_go(reward_list, gamma=0.99):\n",
    "    \"\"\"calculate discounted reward\"\"\"\n",
    "    reward_arr = np.array(reward_list)\n",
    "    for i in range(len(reward_arr) - 2, -1, -1):\n",
    "        # G_t = r_t + γ·r_t+1 + ... = r_t + γ·G_t+1\n",
    "        reward_arr[i] += gamma * reward_arr[i + 1]\n",
    "    # normalize episode rewards\n",
    "    reward_arr -= np.mean(reward_arr)\n",
    "    reward_arr /= np.std(reward_arr)\n",
    "    return reward_arr\n",
    "\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('Pong-v0')\n",
    "obs_dim = 80 * 80\n",
    "act_dim = env.action_space.n\n",
    "logger.info('obs_dim {}, act_dim {}'.format(obs_dim, act_dim))\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "# 4. 请参考课堂Demo构建 agent，嵌套Model, PolicyGradient, Agent\n",
    "#\n",
    "model = Model(act_dim=act_dim)\n",
    "alg = PolicyGradient(model, lr=LEARNING_RATE)\n",
    "agent = Agent(alg, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "# if os.path.exists('./model.ckpt'):\n",
    "#     agent.restore('./model.ckpt')\n",
    "\n",
    "for i in range(20000):\n",
    "    obs_list, action_list, reward_list = run_episode(env, agent)\n",
    "    # if i % 10 == 0:\n",
    "    #     logger.info(\"Train Episode {}, Reward Sum {}.\".format(i, \n",
    "    #                                         sum(reward_list)))\n",
    "\n",
    "    batch_obs = np.array(obs_list)\n",
    "    batch_action = np.array(action_list)\n",
    "    batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "    agent.learn(batch_obs, batch_action, batch_reward)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        total_reward = evaluate(env, agent, render=False)\n",
    "        logger.info('Episode {}, Test reward: {}'.format(i + 1, \n",
    "                                            total_reward))\n",
    "\n",
    "# save the parameters to ./model.ckpt\n",
    "agent.save('./model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
